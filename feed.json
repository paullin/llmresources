{
    "version": "https://jsonfeed.org/version/1",
    "title": "llmresources",
    "description": "",
    "home_page_url": "https://llmresources.org",
    "feed_url": "https://llmresources.org/feed.json",
    "user_comment": "",
    "icon": "https://llmresources.org/media/website/logo-3.png",
    "author": {
        "name": "Paul Lin"
    },
    "items": [
        {
            "id": "https://llmresources.org/genai-tutorial/",
            "url": "https://llmresources.org/genai-tutorial/",
            "title": "系列教程：生成式 AI 导论",
            "summary": "\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://llmresources.org/media/posts/17/GenAi-tutorial-4.webp\" height=\"2000\" width=\"3000\" alt=\"生成式 AI\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://llmresources.org/media/posts/17/responsive/GenAi-tutorial-4-xs.webp 300w ,https://llmresources.org/media/posts/17/responsive/GenAi-tutorial-4-sm.webp 480w ,https://llmresources.org/media/posts/17/responsive/GenAi-tutorial-4-md.webp 768w\">\n      <figcaption>生成式 AI</figcaption>\n    </figure>\n\n  <p>\n    摘要：该系列来源于油管上同名热门视频《生成式 AI 导论 (2024)》的文字稿，教授者是台湾计算机科学家、国立台湾大学电机工程学系教授李宏毅。\n  </p>\n",
            "content_html": "\n    <figure class=\"post__image post__image--center\">\n      <img decoding=\"async\" loading=\"lazy\" src=\"https://llmresources.org/media/posts/17/GenAi-tutorial-4.webp\" height=\"2000\" width=\"3000\" alt=\"生成式 AI\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://llmresources.org/media/posts/17/responsive/GenAi-tutorial-4-xs.webp 300w ,https://llmresources.org/media/posts/17/responsive/GenAi-tutorial-4-sm.webp 480w ,https://llmresources.org/media/posts/17/responsive/GenAi-tutorial-4-md.webp 768w\">\n      <figcaption>生成式 AI</figcaption>\n    </figure>\n\n  <p>\n    摘要：该系列来源于油管上同名热门视频《生成式 AI 导论 (2024)》的文字稿，教授者是台湾计算机科学家、国立台湾大学电机工程学系教授李宏毅。\n  </p>\n\n\n  <p>\n    课程内容深入浅出，幽默风趣，将很多艰深晦涩的专业名词用大白话讲得明明白白。同时提供了很多参考学习的论文，是学习 AI 的最佳入门材料。\n  </p>\n\n  <div class=\"post__toc\">\n    <h3>目录</h3>\n    <ul>\n      <li><a href=\"#xi-lie-wen-zhang\">系列文章</a></li><li><a href=\"#can-kao-lun-wen\">参考论文</a></li><li><a href=\"#shu-yu-biao\">术语表</a></li><li><a href=\"#ke-jian-xia-zai\">课件下载</a></li>\n    </ul>\n  </div>\n  \n\n    <h2 id=\"xi-lie-wen-zhang\">\n      系列文章\n    </h2>\n\n  <p>\n    <a href=\"https://llmresources.org/what-is-generative-ai/\" target=\"_blank\">第一节 生成式 AI 是什么？</a>\n  </p>\n<hr class=\"separator separator--dots\" />\n\n  <p>\n    <strong>版权声明</strong>：本文内容来源于网上同名系列视频《生成式 AI 导论 (2024)》，经大模型翻译和人工校对修正。内容版权归属李宏毅老师所有。如需转载请保留来源链接 (https://llmresources.org)。\n  </p>\n<hr class=\"separator separator--dots\" />\n\n    <h2 id=\"can-kao-lun-wen\">\n      参考论文\n    </h2>\n\n  <ul>\n    <li>无</li>\n  </ul>\n\n    <h2 id=\"shu-yu-biao\">\n      术语表\n    </h2>\n\n  <ul>\n    <li>该系列所有涉及到的术语表，请访问<a href=\"https://llmresources.org/glossary/\" target=\"_blank\">《术语表》</a>页面。</li>\n  </ul>\n\n    <h2 id=\"ke-jian-xia-zai\">\n      课件下载\n    </h2>\n\n  <ul>\n    <li><a href=\"https://speech.ee.ntu.edu.tw/~hylee/genai/2024-spring-course-data/0223/0223_intro_gai.pdf\"  class=\"extlink extlink-icon-1\"  rel=\"nofollow noopener\">第一节课课件</a></li>\n  </ul>",
            "author": {
                "name": "Paul Lin"
            },
            "tags": [
                   "生成式 AI 导论",
                   "全站精选"
            ],
            "date_published": "2025-04-17T11:38:02+08:00",
            "date_modified": "2025-04-18T21:21:46+08:00"
        },
        {
            "id": "https://llmresources.org/glossary/",
            "url": "https://llmresources.org/glossary/",
            "title": "术语表",
            "summary": "\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://llmresources.org/media/posts/16/glossary.webp\" height=\"600\" width=\"900\" alt=\"Glossary | 术语表\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://llmresources.org/media/posts/16/responsive/glossary-xs.webp 300w ,https://llmresources.org/media/posts/16/responsive/glossary-sm.webp 480w ,https://llmresources.org/media/posts/16/responsive/glossary-md.webp 768w\">\n      <figcaption>图片来源：freepik.com</figcaption>\n    </figure>\n\n  <p>\n    <strong>说明</strong>\n  </p>\n\n  <p>\n    本文所列术语及其解释来源于李宏毅老师系列视频《<a href=\"https://llmresources.org/tags/introduction-gen-ai/\" target=\"_blank\" class=\"\">生成式 AI</a>》的课程内容。如果想进一步学习，可通过 Wikipedia 等渠道获取更详细的解释。\n  </p>\n",
            "content_html": "\n    <figure class=\"post__image post__image--center\">\n      <img decoding=\"async\" loading=\"lazy\" src=\"https://llmresources.org/media/posts/16/glossary.webp\" height=\"600\" width=\"900\" alt=\"Glossary | 术语表\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://llmresources.org/media/posts/16/responsive/glossary-xs.webp 300w ,https://llmresources.org/media/posts/16/responsive/glossary-sm.webp 480w ,https://llmresources.org/media/posts/16/responsive/glossary-md.webp 768w\">\n      <figcaption>图片来源：freepik.com</figcaption>\n    </figure>\n\n  <p>\n    <strong>说明</strong>\n  </p>\n\n  <p>\n    本文所列术语及其解释来源于李宏毅老师系列视频《<a href=\"https://llmresources.org/tags/introduction-gen-ai/\" target=\"_blank\" class=\"\">生成式 AI</a>》的课程内容。如果想进一步学习，可通过 Wikipedia 等渠道获取更详细的解释。\n  </p>\n\n\n    <h2 id=\"qren-gong-zhi-nengartificial-intelligence-ai\">\n      Q：人工智能（Artificial Intelligence, AI）\n    </h2>\n\n  <p>\n    A：人工智能的目标是让机器展现“智慧”，它致力于使机器具备类似人类的智能行为，能够完成诸如学习、推理、解决问题、理解自然语言、识别图像等复杂任务，通过模拟和扩展人类智能的方式，让机器在各种领域发挥作用，为人类生活和工作提供便利与支持。<br>\n  </p>\n\n    <h2 id=\"qsheng-cheng-shi-ren-gong-zhi-neng-generative-ai\">\n      Q：生成式人工智能 (Generative AI)\n    </h2>\n\n  <p>\n    A：生成式人工智能是让机器产生复杂有结构物件的人工智能技术，这些物件包括文章、影像、语音等。其需要从近乎无穷的可能组合中找出合适的结果，比如创作一篇特定主题的文章、生成一幅特定场景的图片等，今日多以深度学习达成。<br>\n  </p>\n\n    <h2 id=\"qji-qi-xue-ximachine-learning\">\n      Q：机器学习（Machine Learning）\n    </h2>\n\n  <p>\n    A：机器学习是机器自动从资料中寻找一个函式的方法。在这个过程中，机器通过对大量数据的学习，自动调整模型的参数，以达到对数据的准确拟合和对未知数据的有效预测。就像通过已知的输入输出数据，让机器自动找出函数中的参数值 ，从而能够处理新的输入数据并给出合理输出。<br>\n  </p>\n\n    <h2 id=\"qshen-du-xue-xideep-learning\">\n      Q：深度学习（Deep Learning）\n    </h2>\n\n  <p>\n    A：深度学习是一种机器学习技术，它基于具有很多层的神经网络。这些神经网络可以自动从大量数据中学习复杂的模式和特征表示。通过构建深度的网络结构，深度学习能够处理图像、语音、文本等复杂数据，在图像识别、语音识别、自然语言处理等领域取得了显著成果。<br>\n  </p>\n\n    <h2 id=\"qlei-shen-jing-wang-luoneural-network\">\n      Q：类神经网络（Neural Network）\n    </h2>\n\n  <p>\n    A：类神经网络是一种模拟人类大脑神经元结构和功能的计算模型，由大量相互连接的节点（神经元）和边组成。这些节点按照层次结构组织，包括输入层、隐藏层和输出层。它通过对数据的学习，调整节点之间连接的权重，从而实现对数据的分类、预测等任务，是深度学习中常用的模型架构。<br>\n  </p>\n\n    <h2 id=\"qmo-xingmodel\">\n      Q：模型（Model）\n    </h2>\n\n  <p>\n    A：在人工智能领域，模型是一种数学或计算的表示形式，它基于数据和算法构建，用于模拟和预测现实世界中的现象或任务。模型包含一系列参数，通过对训练数据的学习来调整这些参数，以实现对输入数据的准确处理和输出，比如用于识别猫和狗的模型、生成文章的语言模型等。<br>\n  </p>\n\n    <h2 id=\"qcan-shuparameter\">\n      Q：参数（Parameter）\n    </h2>\n\n  <p>\n    A：参数是模型中的可调整变量，它们决定了模型的具体行为和功能。不同的参数值会导致模型对相同输入产生不同的输出。在机器学习和深度学习中，模型训练的过程就是寻找最优参数值的过程，以使得模型在处理任务时达到最佳性能，如线性函数$y = ax + b$中的$a$和$b$就是参数。<br>\n  </p>\n\n    <h2 id=\"qxun-liantraininglearning\">\n      Q：训练（Training，Learning）\n    </h2>\n\n  <p>\n    A：训练是机器学习和深度学习中的关键环节，指让模型通过对大量训练数据的学习，不断调整自身参数，以提高对数据模式的理解和任务处理能力的过程。在训练过程中，模型会根据数据的特征和标签信息，利用特定的算法来优化参数，使得模型的输出结果尽可能接近真实值，从而提升模型性能。<br>\n  </p>\n\n    <h2 id=\"qce-shitestinginference\">\n      Q：测试（Testing，Inference）\n    </h2>\n\n  <p>\n    A：测试是在模型训练完成后，使用一组未参与训练的数据对模型进行评估的过程，也叫推理。通过测试，可以了解模型对新数据的适应能力和处理准确性，判断模型是否能够泛化到不同的数据样本上。在测试时，模型根据输入数据，利用已训练好的参数进行计算，得出输出结果，以检验模型的性能和效果。 <br>\n  </p>\n\n    <h2 id=\"qfen-leiclassification\">\n      Q：分类（Classification）\n    </h2>\n\n  <p>\n    A：分类是一种常见的机器学习任务，其目的是将输入数据划分到预先定义好的不同类别中。模型会根据输入数据的特征，学习到不同类别之间的差异，从而对新的输入数据进行判断和归类。例如垃圾邮件侦测、猫狗分类器等应用，就是通过分类模型将数据准确地分到相应类别中。<br>\n  </p>\n\n    <h2 id=\"qhui-guiregression\">\n      Q：回归（Regression）\n    </h2>\n\n  <p>\n    A：回归是机器学习中的一种预测任务，主要用于预测连续型的数值输出。与分类任务不同，回归模型的目标是找到输入变量和输出变量之间的关系，通常用一个数学函数来表示。通过对训练数据的学习，模型可以预测出与输入数据对应的连续数值结果，比如预测房价、股票价格走势等。 <br>\n  </p>\n\n    <h2 id=\"qyu-yan-mo-xinglanguage-model\">\n      Q：语言模型（Language Model）\n    </h2>\n\n  <p>\n    A：语言模型是一种基于机器学习或深度学习技术构建的模型，用于处理自然语言任务。它可以根据给定的文本序列预测下一个可能出现的单词或短语，从而生成连贯的文本内容。语言模型通过学习大量文本数据中的语言模式和规律，具备理解和生成自然语言的能力，像ChatGPT就是一种语言模型。<br>\n  </p>\n\n    <h2 id=\"qsheng-cheng-ce-luegeneration-strategy\">\n      Q：生成策略（Generation Strategy）\n    </h2>\n\n  <p>\n    A：生成策略是指生成式人工智能在生成复杂有结构物件时所采用的方法和规则。不同的生成任务和模型会使用不同的生成策略，例如自回归生成就是一种常见的生成策略，它依照某种固定顺序依序生成较小的单位（如文字、像素等），逐步构建出完整的复杂物件。<br>\n  </p>\n\n    <h2 id=\"qzi-hui-gui-sheng-chengautoregressive-generation\">\n      Q：自回归生成（Autoregressive Generation）\n    </h2>\n\n  <p>\n    A：自回归生成是生成式人工智能中的一种生成策略，它按照一定顺序，根据已生成的部分来预测并生成下一个元素。在文本生成中，会根据前文已生成的单词预测下一个单词；在图像生成中，则根据已生成的像素预测下一个像素。通过不断重复这个过程，逐步生成完整的文章、图像等复杂有结构的物件。<br>\n  </p>\n\n    <h2 id=\"qtransformer\">\n      Q：Transformer\n    </h2>\n\n  <p>\n    A：Transformer是一种类神经网络架构，在自然语言处理和其他领域有着广泛应用。它采用了注意力机制，能够有效处理输入序列中不同位置元素之间的关系，捕捉长序列中的依赖信息。相比传统的循环神经网络和卷积神经网络，Transformer在处理长文本、提高模型并行计算能力等方面具有优势，许多先进的语言模型如ChatGPT都基于Transformer架构。&nbsp;\n  </p>",
            "author": {
                "name": "Paul Lin"
            },
            "tags": [
                   "术语表"
            ],
            "date_published": "2025-04-17T10:33:04+08:00",
            "date_modified": "2025-04-18T21:09:33+08:00"
        },
        {
            "id": "https://llmresources.org/what-is-generative-ai/",
            "url": "https://llmresources.org/what-is-generative-ai/",
            "title": "生成式 AI 是什么？",
            "summary": "\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://llmresources.org/media/posts/13/cover.webp\" height=\"1001\" width=\"1500\" alt=\"人工智能\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://llmresources.org/media/posts/13/responsive/cover-xs.webp 300w ,https://llmresources.org/media/posts/13/responsive/cover-sm.webp 480w ,https://llmresources.org/media/posts/13/responsive/cover-md.webp 768w\">\n      <figcaption>来源：freepik.com</figcaption>\n    </figure>\n\n  <p>\n    摘要：本节课从概念层面对几个关键的技术名词定义和关系进行了清晰的界定。包括人工智能与生成式人工智能、机器学习与深度学习、模型与参数、训练与推理。并一针见血地指出大(语言)模型的本质就是在做「文字接龙」的游戏。\n  </p>\n",
            "content_html": "\n    <figure class=\"post__image post__image--center\">\n      <img decoding=\"async\" loading=\"lazy\" src=\"https://llmresources.org/media/posts/13/cover.webp\" height=\"1001\" width=\"1500\" alt=\"人工智能\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://llmresources.org/media/posts/13/responsive/cover-xs.webp 300w ,https://llmresources.org/media/posts/13/responsive/cover-sm.webp 480w ,https://llmresources.org/media/posts/13/responsive/cover-md.webp 768w\">\n      <figcaption>来源：freepik.com</figcaption>\n    </figure>\n\n  <p>\n    摘要：本节课从概念层面对几个关键的技术名词定义和关系进行了清晰的界定。包括人工智能与生成式人工智能、机器学习与深度学习、模型与参数、训练与推理。并一针见血地指出大(语言)模型的本质就是在做「文字接龙」的游戏。\n  </p>\n\n\n  <div class=\"post__toc\">\n    <h3>目录</h3>\n    <ul>\n      <li><a href=\"#zheng-wen\">正文</a></li><li><a href=\"#can-kao-lun-wen\">参考论文</a></li><li><a href=\"#shu-yu-biao\">术语表</a></li><li><a href=\"#ke-jian-xia-zai\">课件下载</a></li>\n    </ul>\n  </div>\n  \n\n    <h2 id=\"zheng-wen\">\n      正文\n    </h2>\n\n  <p>\n    好，我们开始上课。第一堂课要告诉大家的是：什么是生成式人工智能。在讲解生成式人工智能之前，或许我们需要先了解什么是人工智能（<mark>Artificial Intelligence，AI</mark>）。\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img decoding=\"async\" loading=\"lazy\" src=\"https://llmresources.org/media/posts/13/chapter01-2.jpg\" height=\"2250\" width=\"4000\" alt=\"人工智能\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://llmresources.org/media/posts/13/responsive/chapter01-2-xs.webp 300w ,https://llmresources.org/media/posts/13/responsive/chapter01-2-sm.webp 480w ,https://llmresources.org/media/posts/13/responsive/chapter01-2-md.webp 768w\">\n      <figcaption> 人工智能是一种目标</figcaption>\n    </figure>\n\n  <p>\n    从名字来看，人工智能似乎是人类创造出来的、机器所展现的智慧，并非人类本身的智慧。然而，智慧究竟是什么呢？当我们说人工智能是让机器展现智慧时，这只是从字面进行解释。实际上，每个人心中对智慧的理解都不尽相同。<br><br>我经常接到邀请去做与AI相关议题的演讲，最近通常会讲ChatGPT。大家普遍认为ChatGPT属于一种人工智能，但也有人持有不同观点。有人觉得会跑来跑去的机器人或者能挑选土豆的电脑才算是人工智能，认为ChatGPT不算。\n  </p>\n\n  <p>\n    由此可见，人工智能并没有一个标准的定义，每个人心中对它的想象都不一样。\n  </p>\n\n  <p>\n    正因如此，在人工智能相关的论文里，几乎很少提及“人工智能”这个词，因为它是一个定义模糊的词汇。不过，无论我们如何定义智慧这个词，<mark>人工智能都可以被看作是一个目标，一个我们想要达成的目标，它不是某一项单一的技术。</mark><br><br>那么，什么是生成式人工智能呢？它通常被翻译成Generating AI，其定义相对明确。\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img decoding=\"async\" loading=\"lazy\" src=\"https://llmresources.org/media/posts/13/chapter01-3.jpg\" height=\"2250\" width=\"4000\" alt=\"GenAI | 生成式人工智能\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://llmresources.org/media/posts/13/responsive/chapter01-3-xs.webp 300w ,https://llmresources.org/media/posts/13/responsive/chapter01-3-sm.webp 480w ,https://llmresources.org/media/posts/13/responsive/chapter01-3-md.webp 768w\">\n      <figcaption>生成式 AI 的定义</figcaption>\n    </figure>\n\n  <p>\n    生成式人工智能是让机器产生<mark>复杂而有结构</mark>的物件，比如文章，文章由一连串文字构成；影像，由一堆像素组成；语音，由一堆取样点组成。如果不了解取样点也没关系，后续讲解语音的生成式AI时会详细说明。\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img decoding=\"async\" loading=\"lazy\" src=\"https://llmresources.org/media/posts/13/chapter01-4.jpg\" height=\"2250\" width=\"4000\" alt=\"GenAI | 生成式人工智能的特点\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://llmresources.org/media/posts/13/responsive/chapter01-4-xs.webp 300w ,https://llmresources.org/media/posts/13/responsive/chapter01-4-sm.webp 480w ,https://llmresources.org/media/posts/13/responsive/chapter01-4-md.webp 768w\">\n      <figcaption>生成式 AI 的特点</figcaption>\n    </figure>\n\n  <p>\n    <mark>所谓“复杂而有结构”，是指复杂程度要达到无法穷举的地步。</mark><br><br>以在ChatGPT上让它写一篇100字、标题为《缝隙的联想》的中文文章为例。\n  </p>\n\n  <p>\n    当人工智能写这样一篇文章时，背后要解决一个极为困难的问题。写一篇100字的文章，可能性有多少呢？假设中文常用字为1000个（实际常用字远超这个数量，此处为方便计算假设），那么100字文章的可能性就是1000的100次方，即10的300次方。\n  </p>\n\n  <p>\n    这是一个大到难以想象的数字，相比之下，宇宙中原子的数目估计只有10的80次方。机器要从这近乎无穷的可能性中挑出一个合理答案作为回复，显然是非常困难的。当机器解决这个生成式AI的问题时，需要从近乎无穷的可能中找出一个恰当的组合。\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img decoding=\"async\" loading=\"lazy\" src=\"https://llmresources.org/media/posts/13/chapter01-5.jpg\" height=\"2250\" width=\"4000\" alt=\"GenAI | 生成式人工智能 | 分类 | Classification\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://llmresources.org/media/posts/13/responsive/chapter01-5-xs.webp 300w ,https://llmresources.org/media/posts/13/responsive/chapter01-5-sm.webp 480w ,https://llmresources.org/media/posts/13/responsive/chapter01-5-md.webp 768w\">\n      <figcaption>分类不属于生成式 AI</figcaption>\n    </figure>\n\n  <p>\n    <br>为了更好地理解生成式人工智能，我们可以反过来看，什么样的问题不属于生成式AI的问题。比如分类（classification）就不是，分类问题是让机器从有限的选项中做选择。\n  </p>\n\n  <p>\n    像Gmail的垃圾邮件侦测功能，收到一封信时，它只有“是垃圾邮件”和“不是垃圾邮件”这两个选项；影像辨识系统在判断一张图片里是猫还是狗时，也只有两个答案。\n  </p>\n\n  <p>\n    <mark>这种从有限选项中做选择的问题属于分类问题，不属于生成式人工智能。</mark>\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img decoding=\"async\" loading=\"lazy\" src=\"https://llmresources.org/media/posts/13/chapter01-6.jpg\" height=\"2250\" width=\"4000\" alt=\"人工智能 vs 生成式人工智能\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://llmresources.org/media/posts/13/responsive/chapter01-6-xs.webp 300w ,https://llmresources.org/media/posts/13/responsive/chapter01-6-sm.webp 480w ,https://llmresources.org/media/posts/13/responsive/chapter01-6-md.webp 768w\">\n      <figcaption>人工智能 vs 生成式人工智能</figcaption>\n    </figure>\n\n  <p>\n    由此可知，生成式人工智能是人工智能的一种，人工智能是一个较为抽象、每个人想象都不太一样的目标，而生成式人工智能是其中一个具体的目标，即让机器产生复杂有结构的物件，如文字、图片、声音等。\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img decoding=\"async\" loading=\"lazy\" src=\"https://llmresources.org/media/posts/13/chapter01-7.jpg\" height=\"2250\" width=\"4000\" alt=\"Machine Learning | 机器学习\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://llmresources.org/media/posts/13/responsive/chapter01-7-xs.webp 300w ,https://llmresources.org/media/posts/13/responsive/chapter01-7-sm.webp 480w ,https://llmresources.org/media/posts/13/responsive/chapter01-7-md.webp 768w\">\n      <figcaption>机器学习的概念</figcaption>\n    </figure>\n\n  <p>\n    在深入探讨生成式人工智能之前，我们来介绍另一个与人工智能经常一起被提及的概念——机器学习（<mark>Machine Learning</mark>）。\n  </p>\n\n  <p>\n    <mark>机器学习的定义很明确，就是让机器自动从资料里找出一个函数。</mark>\n  </p>\n\n  <p>\n    举个具体例子，在初中数学里，我们学过这样的问题：给定一个函数，输出为y，输入为x，函数表示为f(x)=ax + b。\n  </p>\n\n  <p>\n    已知当x=4时，y=5；当x=2时，y=-1，求a和b的值。经过运算可以得出a=3，b=-7 。\n  </p>\n\n  <p>\n    在机器学习领域，a和b这样的未知数被称为参数（<mark>parameter</mark>）。有了这两个参数后，代入新的x值就能算出y。机器学习与解这类初中数学问题的不同之处在于，初中时我们靠人力计算参数，而机器学习是通过一系列方法自动算出参数。这是因为实际面临的问题往往比f(x)=ax + b复杂得多。\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img decoding=\"async\" loading=\"lazy\" src=\"https://llmresources.org/media/posts/13/chapter01-8.jpg\" height=\"2250\" width=\"4000\" alt=\"Machine Learning | Training | Inference | 训练 | 推理\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://llmresources.org/media/posts/13/responsive/chapter01-8-xs.webp 300w ,https://llmresources.org/media/posts/13/responsive/chapter01-8-sm.webp 480w ,https://llmresources.org/media/posts/13/responsive/chapter01-8-md.webp 768w\">\n      <figcaption>机器学习中的训练与推理</figcaption>\n    </figure>\n\n  <p>\n    假设让机器学会分辨一张图片里是猫还是狗，就需要一个函数f(图片)，其输入是一张图片，输出只有“猫”和“狗”两种可能。\n  </p>\n\n  <p>\n    这样的函数非常复杂，绝不是简单的ax + b就能实现，它可能包含上万个参数，这里用abcd等符号表示，但实际上符号远远不够。\n  </p>\n\n  <p>\n    <strong><mark>这种带有大量未知参数的函数又被称为模型。</mark></strong>在不同文献中，“模型”这个词汇的含义可能不同，在本课程中，当提到模型时，指的就是这种带有大量未知参数的函数。<br><br>给出题目条件，如输入白色的猫，输出应该是“猫”；输入棕色的狗，输出是“狗”等。机器学习技术就能在这些输入输出条件的限制下，帮助找出这上万个参数。\n  </p>\n\n  <p>\n    找出这些参数的过程又叫做训练（training）或学习（learning）。这些帮助找出参数的输入输出条件限制就是训练资料。找出参数后，将其代入模型，就知道函数的具体形式。这时，输入一张新的图片，比如正在打电脑的猫，看看机器会给出什么输出，这个过程叫做测试（Testing）或推论（Inference）。<br><br>在机器学习领域，通常用类神经网络（<mark>Neuronetwork</mark>）来表示这种带有上万个参数的函数。有人认为类神经网络和人类大脑有关，是模仿人类大脑学习的，其实这是一种误解。\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img decoding=\"async\" loading=\"lazy\" src=\"https://llmresources.org/media/posts/13/chapter01-9.jpg\" height=\"2250\" width=\"4000\" alt=\"Deep Learning | 深度学习\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://llmresources.org/media/posts/13/responsive/chapter01-9-xs.webp 300w ,https://llmresources.org/media/posts/13/responsive/chapter01-9-sm.webp 480w ,https://llmresources.org/media/posts/13/responsive/chapter01-9-md.webp 768w\">\n      <figcaption>深度学习是一种机器学习技术</figcaption>\n    </figure>\n\n  <p>\n    类神经网络本质上就是一个有大量参数的函数，当用类神经网络来描述这个函数，并解出这些参数时，所运用的技术就是深度学习（<mark>deep learning</mark>）。所以，深度学习是机器学习的一种。当然，描述函数还有其他方法，但使用类神经网络来描述就是在进行深度学习。\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img decoding=\"async\" loading=\"lazy\" src=\"https://llmresources.org/media/posts/13/chapter01-10.jpg\" height=\"2250\" width=\"4000\" alt=\"(生成式)人工智能与机器学习的关系\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://llmresources.org/media/posts/13/responsive/chapter01-10-xs.webp 300w ,https://llmresources.org/media/posts/13/responsive/chapter01-10-sm.webp 480w ,https://llmresources.org/media/posts/13/responsive/chapter01-10-md.webp 768w\">\n      <figcaption>(生成式)人工智能与机器学习的关系</figcaption>\n    </figure>\n\n  <p>\n    机器学习是一种手段，它与生成式人工智能有交集，也有各自独立的部分。\n  </p>\n\n  <p>\n    生成式人工智能既可以用机器学习来解决，也可以用非机器学习的方法解决；机器学习也并非只能解决生成式人工智能的问题，还能解决分类等其他问题。深度学习是机器学习的一种。\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img decoding=\"async\" loading=\"lazy\" src=\"https://llmresources.org/media/posts/13/chapter01-11.jpg\" height=\"2250\" width=\"4000\" alt=\"简化版的概念理解\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://llmresources.org/media/posts/13/responsive/chapter01-11-xs.webp 300w ,https://llmresources.org/media/posts/13/responsive/chapter01-11-sm.webp 480w ,https://llmresources.org/media/posts/13/responsive/chapter01-11-md.webp 768w\">\n      <figcaption>简化版的概念理解</figcaption>\n    </figure>\n\n  <p>\n    在网络文章中，关于生成式人工智能、深度学习、机器学习的关系，常见的一种表示是把生成式人工智能放在机器学习里面，这种说法也能接受。因为生成式人工智能是一个非常困难的问题，或许其他手段难以达到令人满意的结果，所以通常会用深度学习技术来实现。\n  </p>\n\n  <p>\n    也有人把生成式人工智能放在深度学习里面，虽然深度学习是一种技术，生成式人工智能是一个目标，但由于实际中常用深度学习技术来实现生成式人工智能，所以这种说法也可以理解。\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img decoding=\"async\" loading=\"lazy\" src=\"https://llmresources.org/media/posts/13/chapter01-12.jpg\" height=\"2250\" width=\"4000\" alt=\"Transformer \"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://llmresources.org/media/posts/13/responsive/chapter01-12-xs.webp 300w ,https://llmresources.org/media/posts/13/responsive/chapter01-12-sm.webp 480w ,https://llmresources.org/media/posts/13/responsive/chapter01-12-md.webp 768w\">\n      <figcaption>Transformer 模型</figcaption>\n    </figure>\n\n  <p>\n    那么，机器学习和深度学习如何解决生成式人工智能的问题呢？\n  </p>\n\n  <p>\n    以ChatGPT为例，它也可以被看作是一个函数，输入是一段文字，输出是给用户的回复。\n  </p>\n\n  <p>\n    <mark>ChatGPT功能强大，能对各种问题做出回应，其背后的函数非常复杂，可能包含上亿个或数十亿个参数。这个带有大量参数的模型是类神经网络的一种，叫做Transformer。</mark>\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img decoding=\"async\" loading=\"lazy\" src=\"https://llmresources.org/media/posts/13/chapter01-13.jpg\" height=\"2250\" width=\"4000\" alt=\"大模型的输入与输出\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://llmresources.org/media/posts/13/responsive/chapter01-13-xs.webp 300w ,https://llmresources.org/media/posts/13/responsive/chapter01-13-sm.webp 480w ,https://llmresources.org/media/posts/13/responsive/chapter01-13-md.webp 768w\">\n      <figcaption>大模型的输入与输出</figcaption>\n    </figure>\n\n  <p>\n    从机器学习的概念来讲，打造ChatGPT这样的人工智能，需要准备大量的输入和输出，让机器学习或深度学习技术找出其中的上亿个参数。\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img decoding=\"async\" loading=\"lazy\" src=\"https://llmresources.org/media/posts/13/chapter01-14.jpg\" height=\"2250\" width=\"4000\" alt=\"AI 画图也是函数求解的过程\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://llmresources.org/media/posts/13/responsive/chapter01-14-xs.webp 300w ,https://llmresources.org/media/posts/13/responsive/chapter01-14-sm.webp 480w ,https://llmresources.org/media/posts/13/responsive/chapter01-14-md.webp 768w\">\n      <figcaption>AI 画图也是函数求解的过程</figcaption>\n    </figure>\n\n  <p>\n    同样的原理也适用于打造能画图的AI，如Stable Diffusion、Midjourney、DALL-E等。这些画图AI也可以看作是函数，输入是一段文字，输出是一张图片，其函数同样复杂，也包含上亿个参数。通过收集大量文字与对应图片的关系，交由深度学习技术找出函数和参数，就能让机器实现输入文字输出图片的功能。<br><br>其实，生成这个概念在以往的机器学习课堂中就有提及。在2019年的机器学习课堂上，我就讲过，在机器学习领域，分类这种从有限答案中选择的问题是大家熟悉的领域，而让机器产生有结构的复杂东西，比如生成图片，难度更大。\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img decoding=\"async\" loading=\"lazy\" src=\"https://llmresources.org/media/posts/13/chapter01-15.jpg\" height=\"2250\" width=\"4000\" alt=\"生成式人工智能的进步\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://llmresources.org/media/posts/13/responsive/chapter01-15-xs.webp 300w ,https://llmresources.org/media/posts/13/responsive/chapter01-15-sm.webp 480w ,https://llmresources.org/media/posts/13/responsive/chapter01-15-md.webp 768w\">\n      <figcaption>生成式人工智能的进步</figcaption>\n    </figure>\n\n  <p>\n    当时我用拟人化的说法表示，如果机器能成功生成，就相当于它学会了创造。那时，我觉得这一领域还很遥远，就像漫画《猎人》里的库拉皮卡不知道何时才能登陆暗黑大陆一样。但现在到了2024年，虽然库拉皮卡离暗黑大陆依然遥远，但在生成式AI领域，我们已经取得了一定进展，可以说见到了“暗黑大陆的守门人”。<br><br>那么，生成式AI面临的挑战是什么呢？按照之前的思路，收集足够多的资料、找出函数就能实现生成式AI。但仔细思考会发现，<mark>训练资料可能永远收集不完</mark>。\n  </p>\n\n  <p>\n    在对生成式AI进行测试时，人类可能会提出各种问题，机器给出的答案可能与训练时完全不同。\n  </p>\n\n  <p>\n    比如，让机器写一篇与《缝隙的联想》有关的文章，这个要求可能从未在训练资料中出现过，如果出现就属于泄题了。这是一个全新的问题，而模型要给出正确答案，就需要创造出训练时从未出现过的内容。\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img decoding=\"async\" loading=\"lazy\" src=\"https://llmresources.org/media/posts/13/chapter01-16.jpg\" height=\"2250\" width=\"4000\" alt=\"生成式人工智能的重点是「创造」\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://llmresources.org/media/posts/13/responsive/chapter01-16-xs.webp 300w ,https://llmresources.org/media/posts/13/responsive/chapter01-16-sm.webp 480w ,https://llmresources.org/media/posts/13/responsive/chapter01-16-md.webp 768w\">\n      <figcaption>生成式人工智能的重点是「创造」</figcaption>\n    </figure>\n\n  <p>\n    如果机器能在测试时产生训练中从未见过的内容，或许可以说它具有某种程度的“创造力”（这里的“创造力”是一种特定的定义，有人可能并不认同这种定义）。像ChatGPT这样的人工智能是如何做到产生从未见过的答案的呢？<br><br><mark>ChatGPT背后的核心原理可以用“文字接龙”四个字概括。</mark>\n  </p>\n\n  <p>\n    原本生成式AI是一个难题，因为一段文句的可能性无穷无尽。但在ChatGPT中，生成答案被拆解成一系列文字接龙的问题。\n  </p>\n\n  <p>\n    比如问“台湾最高的山是哪座”，<mark>ChatGPT并不是直接生成完整答案，而是把这个句子当作未完成的句子，预测后面接哪个字合理?</mark>\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img decoding=\"async\" loading=\"lazy\" src=\"https://llmresources.org/media/posts/13/chapter01-17.jpg\" height=\"2250\" width=\"4000\" alt=\"大模型完成任务靠的是「文字接龙」\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://llmresources.org/media/posts/13/responsive/chapter01-17-xs.webp 300w ,https://llmresources.org/media/posts/13/responsive/chapter01-17-sm.webp 480w ,https://llmresources.org/media/posts/13/responsive/chapter01-17-md.webp 768w\">\n      <figcaption>大模型完成任务靠的是「文字接龙」</figcaption>\n    </figure>\n\n  <p>\n    比如接“玉”，然后把“玉”贴到原来句子后面，继续预测“玉”后面接哪个字合理，比如接“山”，当机器认为句子结束时，就输出结<s></s>束符号，这样就得到了“玉山”这个答案。\n  </p>\n\n  <p>\n    能够进行文字接龙的模型叫做语言模型。将生成完整答案的问题转化为文字接龙问题有很大好处，因为文字接龙的答案是有限的。中文常用字大概三四千个，这样一来，<mark>原本复杂的生成式AI问题就变成了一系列分类问题</mark>，而从有限选项中选择答案是人类擅长的，所以生成文章等难题就变得可解。\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img decoding=\"async\" loading=\"lazy\" src=\"https://llmresources.org/media/posts/13/chapter01-18.jpg\" height=\"2250\" width=\"4000\" alt=\"语言模型\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://llmresources.org/media/posts/13/responsive/chapter01-18-xs.webp 300w ,https://llmresources.org/media/posts/13/responsive/chapter01-18-sm.webp 480w ,https://llmresources.org/media/posts/13/responsive/chapter01-18-md.webp 768w\">\n      <figcaption>语言模型</figcaption>\n    </figure>\n\n  <p>\n    不过，语言模型只是生成式人工智能的其中一项技术，生成并不一定要采用文字接龙的方式，还有其他策略。\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img decoding=\"async\" loading=\"lazy\" src=\"https://llmresources.org/media/posts/13/chapter01-19.jpg\" height=\"2250\" width=\"4000\" alt=\"Autoregressive Generation | 自回归生成\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://llmresources.org/media/posts/13/responsive/chapter01-19-xs.webp 300w ,https://llmresources.org/media/posts/13/responsive/chapter01-19-sm.webp 480w ,https://llmresources.org/media/posts/13/responsive/chapter01-19-md.webp 768w\">\n      <figcaption>Autoregressive Generation</figcaption>\n    </figure>\n\n  <p>\n    <mark>把复杂物件拆解成较小单位，再按照固定顺序生成这些小单位的策略，叫做autoregressive generation，ChatGPT采用的就是这种方式。</mark>\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img decoding=\"async\" loading=\"lazy\" src=\"https://llmresources.org/media/posts/13/chapter01-20.jpg\" height=\"2250\" width=\"4000\" alt=\"生成图像的过程也是用 Autoregressive Generation\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://llmresources.org/media/posts/13/responsive/chapter01-20-xs.webp 300w ,https://llmresources.org/media/posts/13/responsive/chapter01-20-sm.webp 480w ,https://llmresources.org/media/posts/13/responsive/chapter01-20-md.webp 768w\">\n      <figcaption>生成图像的过程也是用 Autoregressive Generation</figcaption>\n    </figure>\n\n  <p>\n    同样，生成图片也可以采用类似像素接龙的方式，OpenAI多年前就打造过影像版的GPT，尝试用像素接龙产生图片，但这种方法并没有流行起来，原因在后续讲解生成策略时会说明。\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img decoding=\"async\" loading=\"lazy\" src=\"https://llmresources.org/media/posts/13/chapter01-21.jpg\" height=\"2250\" width=\"4000\" alt=\"生成式人工智能由来已久\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://llmresources.org/media/posts/13/responsive/chapter01-21-xs.webp 300w ,https://llmresources.org/media/posts/13/responsive/chapter01-21-sm.webp 480w ,https://llmresources.org/media/posts/13/responsive/chapter01-21-md.webp 768w\">\n      <figcaption>生成式人工智能由来已久</figcaption>\n    </figure>\n\n  <p>\n    生成式人工智能并非现在才出现，一直有人在研究。2015年我开设的第一堂机器学习课程，当时课程名称叫《机器学习及其深层与结构化》，强调了两个重要技术，一个是深层学习（即现在所说的深度学习，当时Deep Learning还没有统一的中文翻译），另一个是结构化学习，也就是现在的生成式AI。\n  </p>\n\n  <p>\n    那时的结构化学习和现在的生成式AI背后的技术已经有很大不同，技术发展非常迅速。虽然生成式AI的概念早就存在，相关应用也早已融入日常生活，比如2006年上线的Google翻译就是生成式AI的一种应用。\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img decoding=\"async\" loading=\"lazy\" src=\"https://llmresources.org/media/posts/13/chapter01-22.jpg\" height=\"2250\" width=\"4000\" alt=\"谷歌翻译也是一种生成式人工智能\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://llmresources.org/media/posts/13/responsive/chapter01-22-xs.webp 300w ,https://llmresources.org/media/posts/13/responsive/chapter01-22-sm.webp 480w ,https://llmresources.org/media/posts/13/responsive/chapter01-22-md.webp 768w\">\n      <figcaption>谷歌翻译也是一种生成式人工智能</figcaption>\n    </figure>\n\n  <p>\n    翻译时，机器需要生成一段文字，而且输入的文句千变万化，正确的翻译可能在训练资料中从未出现过，所以翻译本身就是一个生成式AI的问题。既然生成式AI早就存在，那为什么现在突然爆火呢？这将在后续课程中为大家剖析。<br><br>由于时间有限，接下来我准备回答slido上面的问题。如果大家想听更具技术含量的内容，可以先到我的Youtube频道上搜索相关视频。在Youtube上搜索我的名字，就能找到我的频道，其中第一部时长80分钟、讲解大型模型的影片，可以帮助大家了解GPT是如何被打造出来的。后续课程我们还会讲解更多这方面的知识。&nbsp;<br><br>\n  </p>\n<hr class=\"separator separator--dots\" />\n\n  <p>\n    <strong>版权声明</strong>：本文内容来源于网上同名系列视频《生成式 AI 导论 (2024)》，经大模型翻译和人工校对修正。内容版权归属李宏毅老师所有。如需转载请保留来源链接 (https://llmresources.org)。\n  </p>\n<hr class=\"separator separator--dots\" />\n\n    <h2 id=\"can-kao-lun-wen\">\n      参考论文\n    </h2>\n\n  <ul>\n    <li>无</li>\n  </ul>\n\n    <h2 id=\"shu-yu-biao\">\n      术语表\n    </h2>\n\n  <p>\n    - 人工智能（Artificial Intelligence, AI）<br>- 生成式人工智能（Generative AI）<br>- 机器学习（Machine Learning）<br>- 深度学习（Deep Learning）<br>- 类神经网络（Neural Network）<br>- 模型（Model）<br>- 参数（Parameter）<br>- 训练（Training，Learning）<br>- 测试（Testing，Inference）<br>- 分类（Classification）<br>- 回归（Regression）<br>- 语言模型（Language Model）<br>- 生成策略（Generation Strategy）<br>- 自回归生成（Autoregressive Generation）<br>- Transformer（类神经网络的一种）&nbsp;<br>\n  </p>\n\n    <h2 id=\"ke-jian-xia-zai\">\n      课件下载\n    </h2>\n\n  <p>\n    <a href=\"https://speech.ee.ntu.edu.tw/~hylee/genai/2024-spring-course-data/0223/0223_intro_gai.pdf\" target=\"_blank\"  class=\"extlink extlink-icon-1\"  rel=\"nofollow noopener\">下载链接</a>\n  </p>",
            "author": {
                "name": "Paul Lin"
            },
            "tags": [
                   "生成式 AI 导论"
            ],
            "date_published": "2025-04-16T22:57:13+08:00",
            "date_modified": "2025-04-18T21:09:37+08:00"
        }
    ]
}
